<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html>
<head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-118361649-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-118361649-1');
</script>
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Merriweather:300,700,700italic,300italic|Open+Sans:700,400" />
<link rel="stylesheet" type="text/css" href="css/article.css"/>
<title>How to visualize decision trees</title>
<!-- META -->
<!-- LinkedIn meta -->
<meta property='og:title' content="How to visualize decision tree"/>
<meta property='og:image' content="http://explained.ai/decision-tree-viz/images/knowledge-TD-3-X.png">
<meta property='og:description' content="Decision trees are the fundamental building block of gradient boosting machines and Random Forests(tm), probably the two most popular machine learning models for structured data. Visualizing decision trees is a tremendous aid when learning how these models work and when interpreting models. Unfortunately, current visualization packages are rudimentary and not immediately helpful to the novice. For example, we couldn't find a library that visualizes how decision nodes split up the feature space. So, we've created a general package (part of the animl library) for scikit-learn decision tree visualization and model interpretation."/>
<meta property='og:url' content="http://explained.ai/decision-tree-viz/index.html"/>

<!-- Facebook meta -->
<meta property="og:type" content="article" />

<!-- Twitter meta -->
<meta name="twitter:title" content="How to visualize decision tree">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@the_antlr_guy">
<meta name="twitter:creator" content="@the_antlr_guy">
<meta name="twitter:description" content="Decision trees are the fundamental building block of gradient boosting machines and Random Forests(tm), probably the two most popular machine learning models for structured data. Visualizing decision trees is a tremendous aid when learning how these models work and when interpreting models. Unfortunately, current visualization packages are rudimentary and not immediately helpful to the novice. For example, we couldn't find a library that visualizes how decision nodes split up the feature space. So, we've created a general package (part of the animl library) for scikit-learn decision tree visualization and model interpretation.">
<meta name="twitter:image" content="http://explained.ai/decision-tree-viz/images/knowledge-TD-3-X.png">
<!-- END META -->
</head>
<body>
<div class="watermark">
<i>Brought to you by <a href='http://explained.ai'>explained.ai</a></i><br>
</div>

<h1>1 How to visualize decision trees</h1>

<p></p>

<p><a href="https://www.linkedin.com/in/terence-parr/">Terence Parr</a> and <a href="https://www.linkedin.com/in/groverpr">Prince Grover</a></p>

<p style="font-size: 80%; line-height:1.1;">(Terence teaches in <a href="https://www.usfca.edu/arts-sciences/graduate-programs/data-science">University of San Francisco's MS in Data Science program</a> and Prince is an alumnus. You might know Terence as the creator of the ANTLR parser generator.)
<p>Please send comments, suggestions, or fixes to <a href="mailto:terence@parr.us">Terence</a>.</p>
</p>



<div id="toc">
<p class="toc_title">Contents</p>
<ul>
	<li><a href="#sec:1.1">Introduction</a>
	<ul>
	</ul>
	</li>
	<li><a href="#sec:1.2">Decision tree review</a>
	<ul>
	</ul>
	</li>
	<li><a href="#sec:1.3">The key elements of decision tree visualization</a>
	<ul>
	</ul>
	</li>
	<li><a href="#sec:1.4">Gallery of decision tree visualizations</a>
	<ul>
	</ul>
	</li>
	<li><a href="#sec:1.5">A comparison to previous state-of-the-art visualizations</a>
	<ul>
	</ul>
	</li>
	<li><a href="#sec:1.6">Our decision tree visualizations</a>
	<ul>
			<li><a href="#sec:1.6.1">Visualizing feature-target space</a></li>
			<li><a href="#sec:1.6.2">It's all about the details</a></li>
			<li><a href="#run-X">Visualizing tree interpretation of a single observation</a></li>
			<li><a href="#sec:1.6.4">Left-to-right orientation</a></li>
			<li><a href="#sec:1.6.5">Simplified non-fancy layout</a></li>

	</ul>
	</li>
	<li><a href="#sec:1.7">What we tried and rejected</a>
	<ul>
	</ul>
	</li>
	<li><a href="#sec:1.8">Code sample</a>
	<ul>
			<li><a href="#sec:1.8.1">Boston regression tree visualization</a></li>
			<li><a href="#sec:1.8.2">Wine classification tree visualization</a></li>

	</ul>
	</li>
	<li><a href="#sec:1.9">Our implementation</a>
	<ul>
			<li><a href="#sec:1.9.1">Shadow trees for scikit decision trees</a></li>
			<li><a href="#sec:1.9.2">Tool mashup</a></li>
			<li><a href="#sec:1.9.3">Vector graphics via SVG</a></li>

	</ul>
	</li>
	<li><a href="#sec:1.10">Lessons learned</a>
	<ul>
	</ul>
	</li>
	<li><a href="#sec:1.11">Future work</a>
	<ul>
	</ul>
	</li>

</ul>
</div>


<p><b>Update July 2020</b> <a href="https://github.com/tlapusan">Tudor Lapusan</a> has become a major contributor to <span class=inlinecode>dtreeviz</span> and, thanks to his work, <span class=inlinecode>dtreeviz</span> can now visualize <a href="https://xgboost.ai/">XGBoost</a> and <a href="https://spark.apache.org">Spark</a> decision trees as well as sklearn.  Beyond what is described in this article, the library now also includes the following features. See <a href="https://github.com/parrt/dtreeviz/blob/master/notebooks/dtreeviz_sklearn_visualisations.ipynb">dtreeviz_sklearn_visualisations.ipynb</a> for examples.</p>
<center>
<table style="">
<thead>
</thead>
<tbody>
<tr>
<td valign=top align="center">A visualization of just the path from the root to a decision tree leaf.</td><td valign=top align="center">An explanation in English how a decision tree makes a prediction for a specific record.</td>
</tr>
<tr>
<td valign=top align="center">

<center>
<center>
<a href="images/dtreeviz_prediction_path1.png">
<img src="images/dtreeviz_prediction_path1.png" width="100%" url="images/dtreeviz_prediction_path1.png">
</a>
</center>

</center>

</td><td valign=top align="center">

<center>
<center>
<a href="images/dtreeviz_prediction_path2.png">
<img src="images/dtreeviz_prediction_path2.png" width="100%" url="images/dtreeviz_prediction_path2.png">
</a>
</center>

</center>

</td>
</tr>
</tbody>
</table>
</center>
<p>Visualizations for purity and distributions for individual leaves.</p>
<center>
<table style="">
<thead>
</thead>
<tbody>
<tr>
<td valign=top align="center">

<center>
<center>
<a href="images/dtreeviz_leaves_1.png">
<img src="images/dtreeviz_leaves_1.png" width="100%" url="images/dtreeviz_leaves_1.png">
</a>
</center>

</center>

</td><td valign=top align="center">

<center>
<center>
<a href="images/dtreeviz_leaves_2.png">
<img src="images/dtreeviz_leaves_2.png" width="100%" url="images/dtreeviz_leaves_2.png">
</a>
</center>

</center>

</td><td valign=top align="center">

<center>
<center>
<a href="images/dtreeviz_leaves_4.png">
<img src="images/dtreeviz_leaves_4.png" width="150%" url="images/dtreeviz_leaves_4.png">
</a>
</center>

</center>

</td>
</tr>
</tbody>
</table>
</center>
<p>
	<hr>
</p>



<h2 id="sec:1.1">1.1 Introduction</h2>


<p>Decision trees are the fundamental building block of <a href="http://explained.ai/gradient-boosting/index.html">gradient boosting machines</a> and <a href="https://en.wikipedia.org/wiki/Random_forest">Random Forests</a>&trade;, probably the two most popular machine learning models for structured data.  Visualizing decision trees is a tremendous aid when learning how these models work and when interpreting models.  Unfortunately, current visualization packages are rudimentary and not immediately helpful to the novice. For example, we couldn't find a library that visualizes how decision nodes split up the feature space. It is also uncommon for libraries to support visualizing a specific feature vector as it weaves down through a tree's decision nodes; we could only find one image showing this.</p>

<p>So, we've created a general package for <a href="https://github.com/scikit-learn/scikit-learn">scikit-learn</a> decision tree visualization and model interpretation, which we'll be using heavily in an upcoming <a href="https://mlbook.explained.ai/">machine learning book</a> (written with <a href="http://www.fast.ai/about/#jeremy">Jeremy Howard</a>).  Here's a sample visualization for a tiny decision tree (click to enlarge):</p>

<p><a href="images/samples/wine-TD-2.svg"><img src="images/samples/wine-TD-2.svg" width="70%"></a></p>

<p>This article demonstrates the results of this work, details the specific choices we made for visualization, and outlines the tools and techniques used in the implementation. The visualization software is part of a nascent Python machine learning library called <a href="https://github.com/parrt/dtreeviz">dtreeviz</a>.  We assume you're familiar with the basic mechanism of decision trees if you're interested in visualizing them, but let's start with a brief summary so that we're all using the same terminology. (If you're not familiar with decision trees, check out <a href="http://course.fast.ai/ml">fast.ai's Introduction to Machine Learning for Coders MOOC</a>.)</p>



<h2 id="sec:1.2">1.2 Decision tree review</h2>


<p>A decision tree is a machine learning model based upon binary trees (trees with at most a left and right child).  A decision tree learns the relationship between observations in a training set, represented as feature vectors <span class=eqnvec>x</span> and target values <span class=eqn>y</span>, by examining and condensing training data into a binary tree of interior nodes and leaf nodes.  (Notation: vectors are in bold and scalars are in italics.)</p>

<p>Each leaf in the decision tree is responsible for making a specific prediction. For regression trees, the prediction is a value, such as price.  For classifier trees, the prediction is a target category (represented as an integer in scikit), such as cancer or not-cancer. A decision tree carves up the feature space into groups of observations that share similar target values and each leaf represents one of these groups.  For regression, similarity in a leaf means a low variance among target values and, for classification, it means that most or all targets are of a single class.</p>

<p>Any path from the root of the decision tree to a specific leaf predictor passes through a series of (internal) decision nodes. Each decision node compares a single feature's value in <span class=eqnvec>x</span>, <span class=eqn>x<sub>i</sub></span>, with a specific <i>split point</i> value learned during training. For example, in a model predicting apartment rent prices, decision nodes would test features such as the number of bedrooms and number of bathrooms. (See <b>Section 1.6.3</b> <i>Visualizing tree interpretation of a single observation</i>.) Even in a classifier with discrete target values, decision nodes still compare numeric <i>feature</i> values because scitkit's decision tree implementation assumes that all features are numeric. Categorical variables must be <a href="https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/">one hot encoded</a>, <a href="https://www.analyticsvidhya.com/blog/2015/11/easy-methods-deal-categorical-variables-predictive-modeling/">binned</a>, <a href="http://forums.fast.ai/t/to-label-encode-or-one-hot-encode/6057">label encoded</a>, etc... </p>

<p>To train a decision node, the model examines a subset of the training observations (or the full training set at the root). The node's feature and split point within that feature space are chosen during training to split the observations into left and right buckets (subsets) to maximize similarity as defined above. (This selection process is generally done through exhaustive comparison of features and  feature values.) The left bucket has observations whose <span class=eqn>x<sub>i</sub></span> feature values are all less than the split point and the right bucket has observations whose <span class=eqn>x<sub>i</sub></span> is greater than the split point.   Tree construction proceeds recursively by creating decision nodes for the left bucket and the right bucket.  Construction stops when some stopping criterion is reached, such as having less than five observations in the node.</p>



<h2 id="sec:1.3">1.3 The key elements of decision tree visualization</h2>


<p>Decision tree visualizations should highlight the following important elements, which we demonstrate below.</p>
<ul>
<li>Decision node <b>feature versus target value</b> distributions (which we call feature-target space in this article). We want to know how separable the target values are based upon the feature and a split point.</li>
<li>Decision node <b>feature name and feature split value</b>.  We need to know which feature each decision node is testing and where in that space the nodes splits the observations.</li>
<li><b>Leaf node purity</b>, which affects our prediction confidence. Leaves with low variance among the target values (regression) or an overwhelming majority target class (classification) are much more reliable predictors.</li>
<li><b>Leaf node prediction value</b>.  What is this leaf actually predicting from the collection of target values?</li>
<li><b>Numbers of samples in decision nodes</b>.  Sometimes it's useful to know where all most of the samples are being routed through the decision nodes.</li>
<li><b>Numbers of samples in leaf nodes</b>.  Our goal is a decision tree with fewer, larger and purer leaves. Nodes with too few samples are possible indications of overfitting.</li>
<li>How a specific feature vector is <b>run down the tree</b> to a leaf. This helps explain why a particular feature vector gets the prediction it does. For example, in a regression tree predicting apartment rent prices, we might find a feature vector routed into a high predicted price leaf because of a decision node that checks for more than three bedrooms.</li>
</ul>


<h2 id="sec:1.4">1.4 Gallery of decision tree visualizations</h2>


<p>Before digging into the previous state-of-the-art visualizations, we'd like to give a little spoiler to show what's possible. This section highlights some samples visualizations we built from scikit regression and classification decision trees on a few data sets. You can also check out the	<a href="https://github.com/parrt/dtreeviz/tree/master/testing/samples">full gallery</a> and <a href="https://github.com/parrt/dtreeviz/blob/master/testing/gen_samples.py">code to generate all samples</a>.</p>
<center>
<table style="">
<thead>
<tr>
<th valign=top width="50%" align="center"></th><th valign=top width="50%" align="center"></th>
</tr>
</thead>
<tbody>
<tr>
<td valign=top align="center"><a href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html#sklearn.datasets.load_wine">Wine</a> 3-class top-down orientation</td><td valign=top align="center"><a href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer">Breast cancer</a> 2-class left-to-right</td>
</tr>
<tr>
<td valign=top align="center"><a href="images/samples/wine-TD-2.svg"><img src="images/samples/wine-TD-2.svg" width="100%"></a></td><td valign=top align="center"><a href="images/samples/breast_cancer-LR-3.svg"><img src="images/samples/breast_cancer-LR-3.svg" width="100%"></a></td>
</tr>
<tr>
<td valign=top align="center"><a href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html#sklearn.datasets.load_iris">Iris</a> 3-class showing a prediction</td><td valign=top align="center"><a href="https://archive.ics.uci.edu/ml/datasets/User+Knowledge+Modeling">User knowledge rating</a> 4-class</td>
</tr>
<tr>
<td valign=top align="center"><a href="images/samples/iris-TD-3-X.svg"><img src="images/samples/iris-TD-3-X.svg" width="100%"></a></td><td valign=top align="center"><a href="images/samples/knowledge-LR-3.svg"><img src="images/samples/knowledge-LR-3.svg" width="100%"></a></td>
</tr>
<tr>
<td valign=top align="center"><a href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits">Digits</a> 10-class</td><td valign=top align="center"><a href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html">Diabetes</a> showing a prediction</td>
</tr>
<tr>
<td valign=top align="center"><a href="images/samples/digits-LR-3.svg"><img src="images/samples/digits-LR-3.svg" width="100%"></a></td><td valign=top align="center"><a href="images/samples/diabetes-TD-3-X.svg"><img src="images/samples/diabetes-TD-3-X.svg" width="100%"></a>	</td>
</tr>
<tr>
<td valign=top align="center"><a href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html">Boston</a> showing a prediction</td><td valign=top align="center"><a href="http://mldata.org/repository/data/viewslug/ratings-of-sweets-sweetrs/">Sweets</a> showing a prediction</td>
</tr>
<tr>
<td valign=top align="center"><a href="images/samples/boston-TD-3-X.svg"><img src="images/samples/boston-TD-3-X.svg" width="100%"></a></td><td valign=top align="center"><a href="images/samples/sweets-TD-3-X.svg"><img src="images/samples/sweets-TD-3-X.svg" width="100%"></a></td>
</tr>
<tr>
<td valign=top align="center">User knowledge rating 4-class non-fancy</td><td valign=top align="center">Diabetes non-fancy</td>
</tr>
<tr>
<td valign=top align="center"><a href="images/samples/knowledge-TD-15-X-simple.svg"><img src="images/samples/knowledge-TD-15-X-simple.svg" width="100%"></a></td><td valign=top align="center"><a href="images/samples/boston-LR-5-X-simple.svg"><img src="images/samples/boston-LR-5-X-simple.svg" width="80%"></a></td>
</tr>
</tbody>
</table>
</center>	


<h2 id="sec:1.5">1.5 A comparison to previous state-of-the-art visualizations</h2>


<p>If you search for &ldquo;visualizing decision trees&rdquo; you will quickly find a <b>Python</b> solution provided by the awesome scikit folks: <a href="http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html">sklearn.tree.export_graphviz</a>.  With more work, you can find visualizations for <a href="http://scikit-learn.org/stable/modules/tree.html">R</a> and even <a href="http://support.sas.com/documentation/cdl/en/vaug/68027/HTML/default/viewer.htm#n0q3i0zwng79kin1kb1zvpo9k312.htm">SAS</a> and <a href="https://www.ibm.com/support/knowledgecenter/en/SS4QC9/com.ibm.solutions.wa_an_overview.2.0.0.doc/wa_discover_viz_expl_insigths_dec_tree.html">IBM</a>. In this section, we collect the various decision tree visualizations we could find and compare them to the visualizations made by our <span class=inlinecode>dtreeviz</span> library. We give a more detailed discussion of our visualizations in the next section.</p>

<p>Let's start with the <a href="http://scikit-learn.org/stable/modules/tree.html">default scitkit visualization</a> of a decision tree on the well-known <a href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html#sklearn.datasets.load_iris">Iris</a> data set (click on images to enlarge).</p>
<center>
<table style="">
<thead>
<tr>
<th valign=top width="50%" align="center"></th><th valign=top width="50%" align="center"></th>
</tr>
</thead>
<tbody>
<tr>
<td valign=top align="center">Default scikit Iris visualization</td><td valign=top align="center">Our <span class=inlinecode>dtreeviz</span> Iris visualization</td>
</tr>
<tr>
<td valign=top align="center"><a href="images/iris-scikit.png"><img src="images/iris-scikit.png" width="100%"></a></td><td valign=top align="center"><a href="images/samples/iris-TD-5.svg"><img src="images/samples/iris-TD-5.svg" width="100%"></a></td>
</tr>
</tbody>
</table>
</center>
<p>The scikit tree does a good job of representing the tree structure, but we have a few quibbles.  The colors aren't the best and it's not immediately obvious why some of the nodes are colored and some aren't.  If the colors represent predicted class for this classifier, one would think just the leaves would be colored because only leaves have predictions. (It turns out the non-colored nodes have no majority prediction.) Including the gini coefficient (certainty score) costs space and doesn't help with interpretation. The count of samples of the  various target classes in each node is somewhat useful, but a histogram would be even better. A target class color legend would be nice.  Finally, using true and false as the edge labels isn't as clear as, say, labels <img style="vertical-align: -0.50687pt;" src="images/eqn-524A50782178998021A88B8CD4C8DCD8-depth000.51.svg"> and <img style="vertical-align: -1.8017265pt;" src="images/eqn-4EDC933D28BFE3F5EFFE94BF892DAD38-depth001.72.svg">. The most obvious difference is that our decision nodes show feature distributions as overlapping stacked-histograms, one histogram per target class. Also, our leaf size is proportional to the number of samples in that leaf.</p>

<p>Scikit uses the same visualization approach for decision tree regressors. For example, here is scikit's visualization using the <a href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html">Boston</a> data set, with <span class=inlinecode>dtreeviz</span>'s version for comparison (click to enlarge images):</p>
<center>
<table style="">
<thead>
<tr>
<th valign=top width="50%" align="center"></th><th valign=top width="50%" align="center"></th>
</tr>
</thead>
<tbody>
<tr>
<td valign=top align="center">Default scikit Boston visualization</td><td valign=top align="center">Our <span class=inlinecode>dtreeviz</span> Boston visualization</td>
</tr>
<tr>
<td valign=top align="center"><a href="images/boston-scikit.png"><img src="images/boston-scikit.png" width="100%"></a></td><td valign=top align="center"><a href="images/samples/boston-TD-3.svg"><img src="images/samples/boston-TD-3.svg" width="100%"></a></td>
</tr>
</tbody>
</table>
</center>
<p>In the scikit tree, it's not immediately clear what the use of color implies, but after studying the image, darker images indicate higher predicted target values. As before, our decision nodes show the feature space distribution, this time using a feature versus target value scatterplot.  The leaves use strip plots to show the target value distribution; leaves with more dots naturally have a higher number of samples.</p>

<p><b>R</b> programmers also have access to a package for <a href="http://blog.revolutionanalytics.com/2013/06/plotting-classification-and-regression-trees-with-plotrpart.html">visualizing decision trees</a>, which gives similar results to scikit but with nicer edge labels:</p>

<p><a href="images/R-tree.png"><img src="images/R-tree.png" width="40%"></a></p>

<p><b>SAS</b> and <b>IBM</b> also provide (non-Python-based) decision tree visualizations.  Starting with SAS, we see that their decision nodes include a bar chart related to the node's sample target values and other details:</p>
<center>
<table style="">
<thead>
<tr>
<th valign=top width="50%" align="center"></th><th valign=top width="50%" align="center"></th>
</tr>
</thead>
<tbody>
<tr>
<td valign=top align="center">SAS visualization</td><td valign=top align="center">SAS visualization (best image quality we could find with numeric features)</td>
</tr>
<tr>
<td valign=top align="center"><a href="images/sas-tree.png"><img src="images/sas-tree.png" width="100%"></a></td><td valign=top align="center"><a href="images/sas-tree2.jpeg"><img src="images/sas-tree2.jpeg" width="100%"></a></td>
</tr>
</tbody>
</table>
</center>
<p>Indicating the size of the left and right buckets via edge width is a nice touch. But, those bar charts are hard to interpret because they have no horizontal axis.  Decision nodes testing categorical variables (left image) have exactly one bar per category, so they must represent simple category counts, rather than feature distributions. For numeric features (right image), SAS decision nodes show a histogram of either target  or feature space (we can't tell from the image). SAS node bar charts / histograms appear to illustrate just target values, which tells us nothing about how the feature space was split.</p>

<p>The SAS tree on the right appears to highlight a path through the decision tree for a specific unknown feature vector, but we couldn't find any other examples from other tools and libraries.  The ability to visualize a specific vector run down the tree does not seem to be generally available.</p>

<p>Moving on to IBM software, here is a nice visualization that also shows decision node category counts as bar charts, from <a href="https://www.ibm.com/support/knowledgecenter/en/SS4QC9/com.ibm.solutions.wa_an_overview.2.0.0.doc/wa_discover_viz_expl_insigths_dec_tree.html">IBM's Watson analytics</a> (on the <a href="https://www.kaggle.com/c/titanic/data">TITANIC</a> data set):</p>

<p><a href="images/ibm-tree.png"><img src="images/ibm-tree.png" width="60%"></a></p>

<p>IBM's earlier <b>SPSS</b> product also had decision tree visualizations:</p>
<center>
<table style="">
<thead>
<tr>
<th valign=top width="50%" align="center"></th><th valign=top width="50%" align="center"></th>
</tr>
</thead>
<tbody>
<tr>
<td valign=top align="center">SPSS visualization</td><td valign=top align="center">SPSS visualization</td>
</tr>
<tr>
<td valign=top align="center"><a href="images/spss-tree.png"><img src="images/spss-tree.png" width="100%"></a></td><td valign=top align="center"><a href="images/spss-tree2.png"><img src="images/spss-tree2.png" width="100%"></a></td>
</tr>
</tbody>
</table>
</center>
<p>These SPSS decision nodes seem to give the same SAS-like bar chart of sample target class counts.</p>

<p>All of the visualizations we encountered from the major players were useful, but we were most inspired by the eye-popping visualizations in <a href="http://www.r2d3.us/visual-intro-to-machine-learning-part-1/">A visual introduction to machine learning</a>, which shows an (animated) decision tree like this:</p>

<p><a href="images/vizml-tree.png"><img src="images/vizml-tree.png" width="75%"></a></p>

<p>This visualization has three unique characteristics over previous work, aside from the animation:</p>
<ul>
<li>the decision nodes show how the feature space is split</li>
<li>the split points for decision nodes are shown visually (as a wedge) in the distribution</li>
<li>the leaf size is proportional to the number of samples in that leaf</li>
</ul>
<p>While that visualization is a hardcoded animation for educational purposes, it points in the right direction.</p>



<h2 id="sec:1.6">1.6 Our decision tree visualizations</h2>


<p>Other than the educational animation in <a href="http://www.r2d3.us/visual-intro-to-machine-learning-part-1/">A visual introduction to machine learning</a>, we couldn't find a decision tree visualization package that illustrates how the feature space is split up at  decision nodes (feature-target space).  This is the critical operation performed during decision tree model training and is what newcomers should focus on, so we'll start by examining decision node visualizations for both classification and regression trees. </p>



<h3 id="sec:1.6.1">1.6.1 Visualizing feature-target space</h3>


<p>Training of a decision node chooses feature <span class=eqn>x<sub>i</sub></span> and split value within <span class=eqn>x<sub>i</sub></span>'s range of values (feature space) to group samples with similar target values into two buckets.  Just to be clear, training involves examining the relationship between features and target values. Unless decision nodes show feature-target space in some way, the viewer cannot see how and why training arrived at the split value. To highlight how decision nodes carve up the feature space, we trained a regressor and classifier with a single (AGE) feature (<a href="https://github.com/parrt/dtreeviz/blob/master/testing/paper_examples.py">code to generate images</a>).  Here's a regressor decision tree trained using a single feature from the Boston data, <span class=inlinecode>AGE</span>, and with node ID labeling turned on for discussion purposes here:</p>

<p><a href="images/boston-TD-AGE.svg"><img src="images/boston-TD-AGE.svg" width="100%"></a></p>

<p>Horizontal dashed lines indicate the target mean for the left and right buckets in decision nodes; a vertical dashed line indicates the split point in feature space. The black wedge highlights the split point and identifies the exact split value. Leaf nodes indicate the target prediction (mean) with a dashed line.</p>

<p>As you can see, each <span class=inlinecode>AGE</span> feature axis uses the same range, rather than zooming in, to make it easier to compare decision nodes. As we descend through decision nodes, the sample <span class=inlinecode>AGE</span> values are boxed into narrower and narrower regions. For example, the <span class=inlinecode>AGE</span> feature space in node 0 is split into the regions of <span class=inlinecode>AGE</span> future space shown in nodes 1 and 8. Node 1's space is then split into the chunks shown in nodes 2 and 5. The prediction leaves are not very pure because training a model on just a single variable leads to a poor model, but this restricted example demonstrates how decision trees carve up feature space.</p>

<p>While a decision tree implementation is virtually identical for both classifier and regressor decision trees, the way we interpret them is very different, so our visualizations are distinct for the two cases.  For a regressor, showing feature-target space is best done with a scatterplot of feature versus target.  For classifiers, however, the target is a category, rather than a number, so we chose to illustrate feature-target space using histograms as an indicator of feature space distributions.  Here's a classifier tree trained on the USER KNOWLEDGE data, again with a single feature (PEG) and with nodes labeled for discussion purposes:</p>

<p><a href="images/knowledge-TD-PEG.svg"><img src="images/knowledge-TD-PEG.svg" width="100%"></a></p>

<p>Ignoring color, the histogram shows the PEG feature space distribution.  Adding color gives us an indication of the relationship between feature space and target class. For example, in node 0 we can see that samples with <span class=inlinecode>very_low</span> target class are clustered at the low end of PEG feature space and samples with <span class=inlinecode>High</span> target class are clustered at the high-end. As with the regressor, the feature space of a left child is everything to the left of the parent's split point in the same  feature space; similarly for the right child.  For example, combining the histograms of nodes 9 and 12 yields the histogram of node 8. We force the horizontal axis range to be the same for all PEG decision nodes so that decision nodes lower in the tree clearly box in narrower regions that are more and more pure.</p>

<p>We use a stacked histogram so that overlap is clear in the feature space between samples with different target classes.  Note that the height in the Y axis of the stacked histogram is the total number of samples from all classes; multiple class counts are stacked on top of each other. </p>

<p>When there are more than four or five classes, the stacked histograms are difficult to read, so we recommend setting the histogram type parameter to <span class=inlinecode>bar</span> not <span class=inlinecode>barstacked</span> in this case.  With high cardinality target categories, the overlapping distributions are harder to visualize and things break down, so we set a limit of 10 target classes. Here's a shallow tree example using the 10-class Digits data set using non-stacked histograms:</p>

<p><a href="images/samples/digits-TD-2.svg"><img src="images/samples/digits-TD-2.svg" width="65%"></a></p>



<h3 id="sec:1.6.2">1.6.2 It's all about the details</h3>


<p>Thus far we've skipped over many of the visual cues and details that we  obsessed over during construction of the library and so we hit the key elements here.</p>

<p>Our classifier tree visualizations use node size to give visual cues about the number of samples associated with each node.  Histograms get proportionally shorter as the number of samples in the node decrease and leaf node diameters get smaller.  The feature space (horizontal axis) is always the same width and the same range for a given feature, which makes it much easier to compare the feature-target spaces of different nodes. The bars of all histograms are the same width in pixels. We use just the start/stop range labels for both horizontal and vertical axes to reduce clutter. </p>

<p><center>
<a href="images/knowledge-dec-node.png">
<img src="images/knowledge-dec-node.png" width="35%" url="images/knowledge-dec-node.png">
</a>
</center>
<center>
<a href="images/knowledge-dec-node2.png">
<img src="images/knowledge-dec-node2.png" width="35%" url="images/knowledge-dec-node2.png">
</a>
</center>
</p>

<p>We use a pie chart for classifier leaves, despite their bad reputation.  For the purpose of indicating purity, the viewer only needs an indication of whether there is a single strong majority category. The viewer does not need to see the exact relationship between elements of the pie chart, which is one key area where pie charts fail. The color of the pie chart majority slice gives the leaf prediction. </p>

<p><center>
<a href="images/knowledge-leaf.png">
<img src="images/knowledge-leaf.png" width="8%" url="images/knowledge-leaf.png">
</a>
</center>
</p>

<p>Turning to regressor trees now, we make sure that the target (vertical) axis of all decision nodes is the same height and the same range to make comparing nodes easier. Regressor feature space (horizontal axis) is always the same width and the same range for a given feature.  We set a low alpha for all scatterplot dots so that increased target value density corresponds to darker color.  </p>

<p><center>
<a href="images/boston-dec-node.png">
<img src="images/boston-dec-node.png" width="30%" url="images/boston-dec-node.png">
</a>
</center>
</p>

<p>Regressor leaves also show the same range vertically for the target space. We use a <a href="https://seaborn.pydata.org/generated/seaborn.stripplot.html">strip plot</a> rather than, say, a box plot, because the strip plot shows the distribution explicitly while implicitly showing the number of samples by the number of dots. (We also write out the number of samples in text for leaves.) The leaf prediction is the distribution center of mass (mean) of the strip plot, which we highlight with a dashed line.</p>

<p><center>
<a href="images/boston-leaf.png">
<img src="images/boston-leaf.png" width="25%" url="images/boston-leaf.png">
</a>
</center>
</p>

<p>There are also a number of miscellaneous details that we think improve the quality of the diagrams:</p>
<ul>
<li>Classifiers include a legend</li>
<li>All colors were handpicked from colorblind safe palettes, one handpicked palette per number of target categories (2 through 10)</li>
<li>We use a gray rather than black for text because it's easier on the eyes</li>
<li>Lines are hairlines</li>
<li>We draw outlines of bars in bar charts and slices in pie charts</li>
</ul>


<h3 id="run-X">1.6.3 Visualizing tree interpretation of a single observation</h3>


<p>To figure out how model training arrives at a specific tree, all of the action is in the feature-space splits of the decision nodes, which we just discussed. Now, let's take a look at visualizing how a specific feature vector yields a specific prediction. The key here is to examine the decisions taken along the path from the root to the leaf predictor node. </p>

<p>Decision-making within a node is straightforward: take the left path if feature <span class=eqn>x<sub>i</sub></span> in test vector <span class=eqnvec>x</span> is less than the split point, otherwise take the right path. To highlight the decision-making process, we have to highlight the comparison operation. For decision nodes along the path to the leaf predictor node, we show an orange wedge at position <span class=eqn>x<sub>i</sub></span> in the horizontal feature space. This makes the comparison easy to see; if the orange wedge is to the left of the black wedge, go left else go right. Decision nodes involved in the prediction process are surrounded by boxes with dashed lines and the child edges are thicker and colored orange. Here are two sample trees showing test vectors (click on images to expand):</p>
<center>
<table style="">
<thead>
<tr>
<th valign=top width="50%" align="center"></th><th valign=top width="50%" align="center"></th>
</tr>
</thead>
<tbody>
<tr>
<td valign=top align="center">KNOWLEDGE data with test vector</td><td valign=top align="center">Diabetes data with test vector</td>
</tr>
<tr>
<td valign=top align="center"><a href="images/samples/knowledge-TD-3-X.svg"><img src="images/samples/knowledge-TD-3-X.svg" width="100%"></a></td><td valign=top align="center"><a href="images/samples/diabetes-TD-3-X.svg"><img src="images/samples/diabetes-TD-3-X.svg" width="100%"></a></td>
</tr>
</tbody>
</table>
</center>
<p>The test vector <span class=eqnvec>x</span> with feature names and values appears below the leaf predictor node (or to the right in left-to-right orientation). The test vector highlights the features used in one or more decision nodes.  When the number of features reaches a threshold of 20 (10 for left-to-right orientation),  test vectors do not show unused features to avoid unwieldly test vectors.</p>



<h3 id="sec:1.6.4">1.6.4 Left-to-right orientation</h3>


<p>Some users have a preference for left-to-right orientation instead of top-down and sometimes the nature of the tree simply flows better left-to-right. Sample feature vectors can still be run down the tree with the left-to-right orientation. Here are some examples (click on the images to enlarge):</p>
<center>
<table style="">
<thead>
<tr>
<th valign=top width="45%" align="center"></th><th valign=top width="45%" align="center"></th>
</tr>
</thead>
<tbody>
<tr>
<td valign=top align="center">Wine</td><td valign=top align="center">Diabetes</td>
</tr>
<tr>
<td valign=top align="center"><a href="images/samples/wine-LR-3.svg"><img src="images/samples/wine-LR-3.svg" width="100%"></a></td><td valign=top align="center"><a href="images/samples/diabetes-LR-3.svg"><img src="images/samples/diabetes-LR-3.svg" width="100%"></a></td>
</tr>
<tr>
<td valign=top align="center">Wine showing a prediction</td><td valign=top align="center">Diabetes showing a prediction</td>
</tr>
<tr>
<td valign=top align="center"><a href="images/samples/wine-LR-2-X.svg"><img src="images/samples/wine-LR-2-X.svg" width="100%"></a></td><td valign=top align="center"><a href="images/samples/diabetes-LR-2-X.svg"><img src="images/samples/diabetes-LR-2-X.svg" width="100%"></a> </td>
</tr>
</tbody>
</table>
</center>


<h3 id="sec:1.6.5">1.6.5 Simplified non-fancy layout</h3>


<p>To evaluate the generality of a decision tree, if it often helps to get a high-level overview of the tree.  This generally means examining things like tree shape and size, but more importantly, it means looking at the leaves.  We'd like to know how many samples each leaf has, how pure the target values are, and just generally where most of the weight of samples falls.   Getting an overview is harder when the visualization is too large and so we provide a &ldquo;non-fancy&rdquo; option that generates smaller visualizations while retaining key leaf information. Here are a sample classifier and a regressor in non-fancy mode with top-down orientation:</p>

<p>	<a href="images/samples/knowledge-TD-15-X-simple.svg"><img src="images/samples/knowledge-TD-15-X-simple.svg" width="55%"></a></p>

<p>	<a href="images/samples/boston-TD-5-X-simple.svg"><img src="images/samples/boston-TD-5-X-simple.svg" width="100%"></a></p>

<p>	</p>




<h2 id="sec:1.7">1.7 What we tried and rejected</h2>


<p>	Those interested in these tree visualizations from a design point of view might find it interesting to read about what we tried and rejected.  Starting with classifiers, we thought that the histograms were a bit blocky and perhaps kernel density estimates would give a more accurate picture. We had decision nodes that looked like this:</p>

<p>		<center>
<a href="images/kde.png">
<img src="images/kde.png" width="50%" url="images/kde.png">
</a>
</center>
</p>

<p>	The problem is that decision nodes with only one or two samples gave extremely misleading distributions:</p>

<p>		<center>
<a href="images/kde-leaf.png">
<img src="images/kde-leaf.png" width="12%" url="images/kde-leaf.png">
</a>
</center>
</p>

<p>	We also experimented using bubble charts instead of histograms for classifier decision nodes:</p>

<p>		<center>
<a href="images/bubble.png">
<img src="images/bubble.png" width="70%" url="images/bubble.png">
</a>
</center>
</p>

<p>	These look really cool but, in the end, histograms are easier to read.</p>

<p>	Turning to regression trees, we considered using box plots to show the distribution of prediction values and also used a simple bar chart to show the number of samples:</p>

<p>		<center>
<a href="images/dual-leaf.png">
<img src="images/dual-leaf.png" width="15%" url="images/dual-leaf.png">
</a>
</center>
</p>

<p>	This dual plot for each leaf is less satisfying than the strip plot we use now. The box plot also doesn't show the distribution of target values nearly as well as a strip plot. Before the strip plot, we just laid out the target values using the sample index value as the horizontal axis:</p>

<p>		<center>
<a href="images/non-strip-plot.png">
<img src="images/non-strip-plot.png" width="30%" url="images/non-strip-plot.png">
</a>
</center>
</p>

<p>	That is misleading because the horizontal axis is usually feature space. We scrunched that into a strip plot.</p>



<h2 id="sec:1.8">1.8 Code sample</h2>


<p>This section gives a sample visualization for the Boston regression data set and the Wine classification data set.  You can also check out the	<a href="https://github.com/parrt/dtreeviz/tree/master/testing/samples">full gallery</a> of sample visualizations and the <a href="https://github.com/parrt/dtreeviz/blob/master/testing/gen_samples.py">code to generate the samples</a>.</p>



<h3 id="sec:1.8.1">1.8.1 Boston regression tree visualization</h3>


<p>	Here is a code snippet to load the Boston data and train a regression tree with a maximum depth of three decision nodes:</p>


<div class="codeblk">boston = load_boston()

X_train = boston.data
y_train = boston.target
testX = X_train[5,:]

regr = tree.DecisionTreeRegressor(max_depth=3)
regr = regr.fit(X_train, y_train)</div>


<p>The code to visualize the tree involves passing the tree model, the training data, feature and target names, and a test vector (if desired):</p>


<div class="codeblk">viz = dtreeviz(regr, X_train, y_train, target_name='price',
               feature_names=boston.feature_names,
               X = testX)
viz.save("boston.svg") # suffix determines the generated image format
viz.view()             # pop up window to display image</div>


<p><a href="images/samples/boston-TD-3-X.svg"><img src="images/samples/boston-TD-3-X.svg" width="45%"></a></p>



<h3 id="sec:1.8.2">1.8.2 Wine classification tree visualization</h3>


<p>	Here is a code snippet to load the Wine data and train a classifier tree with a maximum depth of three decision nodes:</p>


<div class="codeblk">clf = tree.DecisionTreeClassifier(max_depth=3)
wine = load_wine()

clf.fit(wine.data, wine.target)</div>


<p>Visualizing a classifier is the same as visualizing a regressor, except that the visualization needs the target class names:</p>


<div class="codeblk">viz = dtreeviz(clf, wine.data, wine.target, target_name='wine',
              feature_names=wine.feature_names,
              class_names=list(wine.target_names))
viz.view()</div>


<p><a href="images/samples/wine-LR-3.svg"><img src="images/samples/wine-LR-3.svg" width="45%"></a></p>

<p>In Jupyter notebooks, the object returned from <span class=inlinecode>dtreeviz()</span> has a <span class=inlinecode>_repr_svg_()</span> function that Jupyter uses to display the object automatically. See <a href="https://github.com/parrt/dtreeviz/blob/master/notebooks/examples.ipynb">notebook of examples</a>.</p>
<div class=aside><b>BUG IN JUPYTER NOTEBOOK</b><br>
Jupyter notebooks currently, as of September 2018, do not display the SVG generated by this library properly. The fonts etc... are all messed up:
<p>	<center>
<a href="images/bad-jupyter-svg.png">
<img src="images/bad-jupyter-svg.png" width="45%" url="images/bad-jupyter-svg.png">
</a>
</center>
</p>

<p>	The good news is that github displays them properly as does <a href="https://jupyterlab.readthedocs.io/en/stable/getting_started/overview.html">JupyterLab</a>.</p>

<p>	Use <span class=inlinecode>Image(viz.topng())</span> to display (poorly) in Juypter notebook or simply call <span class=inlinecode>viz.view()</span>, which will pop up a window that displays things properly.</p>

</div>



<h2 id="sec:1.9">1.9 Our implementation</h2>


<p>This project was very frustrating with lots of programming deadends, fiddling with parameters, working around bugs/limitations in tools and libraries, and creatively mashing up a bunch of existing tools. The only fun part was the (countless) sequence of experiments in visual design.  We pushed through because it seemed likely that the machine learning community would find these visualization as useful as we will. This project represents about two months of trudging through stackoverflow, documentation, and hideous graphics programming.</p>

<p>At the highest level, we used <a href="https://matplotlib.org/">matplotlib</a> to generate images for decision and leaf nodes and combined them into a tree using the venerable <a href="http://graphviz.org/">graphviz</a>. We also used HTML labels extensively in the graphviz tree description for layout and font specification purposes. The single biggest headache was convincing all components of our solution to produce high-quality vector graphics.</p>

<p>Our initial coding experiments led us to create a shadow tree wrapping the decision trees created by scikit, so let's start with that.</p>



<h3 id="sec:1.9.1">1.9.1 Shadow trees for scikit decision trees</h3>


<p>The decision trees for classifiers and regressors from scikit-learnare built for efficiency, not necessarily ease of tree walking or extracting node information. We created <a href="https://github.com/parrt/dtreeviz/blob/master/dtreeviz/shadow.py">dtreeviz.shadow.ShadowDecTree</a> and <span class=inlinecode>dtreeviz.shadow.ShadowDecTreeNode</span> classes as an easy-to-use (traditional binary tree) wrapper for all tree information.  Here's how to create a shadow tree from a scikit classifier or regressor tree model: </p>


<div class="codeblk">shadow_tree = ShadowDecTree(tree_model, X_train, y_train, feature_names, class_names)</div>

						
<p>The shadow tree/node classes have a lot of methods that could be useful to other libraries and tools that need to walk scikit decision trees. For example, <span class=inlinecode>predict()</span> not only runs a feature vector through the tree but also returns the path of visited nodes. The samples associated with any particular node can be had through <span class=inlinecode>node_samples()</span>. </p>

<p><center>
<a href="images/ShadowDecTree.png">
<img src="images/ShadowDecTree.png" width="44%" url="images/ShadowDecTree.png">
</a>
</center>
 <center>
<a href="images/ShadowDecTreeNode.png">
<img src="images/ShadowDecTreeNode.png" width="49%" url="images/ShadowDecTreeNode.png">
</a>
</center>
</p>



<h3 id="sec:1.9.2">1.9.2 Tool mashup</h3>


<p>Graphviz dot tree layout language is very useful for getting decent tree layouts if you know all of the tricks, such as getting left children to appear to the left of right children with interconnecting hidden graph edges.  For example, if you have two leaves, <span class=inlinecode>leaf4</span> and <span class=inlinecode>leaf5</span>, that must appear left to right on the same level, here is the graphviz magic:</p>


<div class="codeblk">LSTAT3 -> leaf4 [penwidth=0.3 color="#444443" label=&lt;>]
LSTAT3 -> leaf5 [penwidth=0.3 color="#444443" label=&lt;>]
{
    rank=same;
    leaf4 -> leaf5 [style=invis]
}</div>


<p>We usually use HTML labels on graphviz nodes rather than just text labels because they give much more control over text display and provide an ability to show tabular data as actual tables. For example, when displaying a test vector run down the tree, the test vector is shown using an HTML table:</p>

<p><center>
<a href="images/X.png">
<img src="images/X.png" width="25%" url="images/X.png">
</a>
</center>
</p>

<p>To generate generate images from graphviz files, we use the <span class=inlinecode>graphviz</span> python package, which ends up <span class=inlinecode>exec</span>ing the <span class=inlinecode>dot</span> binary executable using one of its utility routines (<span class=inlinecode>run()</span>).  Occasionally, we used slightly different parameters on the <span class=inlinecode>dot</span> command and so we just directly call <span class=inlinecode>run()</span> like this for flexibility:</p>


<div class="codeblk">cmd = ["dot", "-Tpng", "-o", filename, dotfilename]
stdout, stderr = run(cmd, capture_output=True, check=True, quiet=False)</div>


<p>We also use the <span class=inlinecode>run()</span> function to execute the <span class=inlinecode>pdf2svg</span> (PDF to SVG conversion) tool, as described in the next section.</p>



<h3 id="sec:1.9.3">1.9.3 Vector graphics via SVG</h3>


<p>We use matplotlib to generate the decision and leaf nodes and, to get the images into a graphviz/dot image, we use HTML graphviz labels and then reference the generated images via <span class=inlinecode>img</span> tags like this:</p>


<div class="codeblk">&lt;img src="/tmp/node3_94806.svg"/></div>


<p>The 94806 number is the process ID, which helps isolate multiple instances of <span class=inlinecode>dtreeviz</span> running on the same machine. Without this, it's possible for multiple processes to overwrite the same temporary files.</p>

<p>Because we wanted scalable, vector graphics, we tried importing SVG images initially but we could not get graphviz to accept those files (pdf neither).  It took us four hours to figure out  that generating and importing SVG were two different things and we needed the following magic incantation on OS X using <span class=inlinecode>--with-librsvg</span>:</p>


<div class="codeblk">$ brew install graphviz --with-librsvg --with-app --with-pango</div>


<p>Originally, when we resorted to generating PNG files from matplotlib, we set the dots per inch (dpi) to be 450 so that they looked okay on high resolution screens like the iMac. Unfortunately, that meant we had to specify the actual size we wanted for the overall tree using an HTML table in graphviz using <span class=inlinecode>width</span> and <span class=inlinecode>height</span> parameters on <span class=inlinecode>&lt;td></span> tags. That cause a lot of trouble because we had to figure out what the aspect ratio was coming out of matplotlib.  Once we moved to SVG files, we unnecessarily parsed the SVG files to get the size for use in the HTML; as we wrote this document we realized  extracting the size information from SVG files was unnecessary.</p>

<p>Unfortunately, graphviz's SVG output simply referenced the node files that we imported, rather than embedding the node images within the overall tree image. This is a very inconvenient form because sending a single tree visualization means sending a zip of files rather than a single file. We spent the time to parse SVG XML and embed all referenced images within a single large meta-SVG file. That worked great and there was much celebration.</p>

<p>Then we noticed that graphviz does not properly handle text in HTML labels when generating SVG. For example, the text of classifier tree legends was cut off and overlapping. Rats.</p>

<p>What finally worked to get a single clean SVG file was first generating a PDF file from graphviz and then converting the PDF to SVG with <span class=inlinecode>pdf2svg</span> (<span class=inlinecode>pdf2cairo</span> also appears to work).</p>

<p>Then we noticed that Jupyter notebook has a bug where it does not display those SVG files properly (see above). Jupyter lab does handle the SVG properly as does github. We added a <span class=inlinecode>topng()</span> method so users of Jupyter notebook can use <span class=inlinecode>Image(viz.topng())</span> to get inline images. Better yet, call <span class=inlinecode>viz.view()</span>, which will pop up a window that displays images properly.</p>

<p>	</p>




<h2 id="sec:1.10">1.10 Lessons learned</h2>


<p>		Sometimes solving a programming problem is less about algorithms and more about working within the constraints and capabilities of the programming ecosystem, such as tools and libraries.  That is definitely the case with this decision tree visualization software. The programming was not hard; it was more a matter of fearlessly bashing our way to victory through an appropriate mashup of graphics tools and libraries.  </p>

<p>		Designing the actual visualization also required a seemingly infinite number of experiments and tweaks. Generating high quality vector-based images also required pathological determination and a trail of dead code left along the circuitous path to success.</p>

<p>	We are definitely not visualization aficionados, but for this specific problem we banged on it until we got effective diagrams.  In <a href="https://www.edwardtufte.com/tufte/courses">Edward Tufte's seminar</a> I learned that you can pack a lot of information into a rich diagram, as long as it's not an arbitrary mishmash; the human eye can resolve lots of details.  We used a number of elements from the design palette to visualize decision trees: color, line thickness, line style, different kinds of plots, size (area, length, graph height, ...), color transparency (alpha), text styles (color, font, bold, italics, size), graph annotations, and visual flow.  All visual elements had to be motivated. For example, we didn't use color just because colors are nice. We used color to  highlight an important dimension (target category) because humans quickly and easily pick out color differences.  Node size differences should also be easily picked out by humans. (is that a kitty cat or lion?), so we used that to indicate leaf size.</p>



<h2 id="sec:1.11">1.11 Future work</h2>


<p>	The visualizations described in this document are part of the <a href="https://github.com/parrt/dtreeviz">dtreeviz</a> machine learning library, which is just getting started. We'll likely moved the <a href="https://github.com/parrt/random-forest-importances">rfpimp</a> permutation importance library into <span class=inlinecode>dtreeviz</span> soon.  At this point, we haven't tested the visualizations on anything but OS X. We'd welcome instructions from programmers on other platforms so that we could include those installation steps in the documentation.</p>

<p>	There are a couple of tweaks we like to do, such as bottom justifying the histograms and classifier trees so that it's easier to compare notes. Also, some of the wedge labels overlap with the axis labels.  Finally, it would be interesting to see what the trees look like with incoming edge thicknesses proportional to the number of samples in that node.	</p>



</body>
</html>

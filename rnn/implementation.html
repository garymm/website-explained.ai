<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html>
<head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-118361649-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-118361649-1');
</script>
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Merriweather:300,700,700italic,300italic|Open+Sans:700,400" />
<link rel="stylesheet" type="text/css" href="css/article.css"/>
<title>An RNN built with matrices and trained with SGD</title>
<!-- META -->
<!-- LinkedIn meta -->
<meta property='og:title' content="Explaining RNNs without neural networks"/>
<meta property='og:image' content="http://explained.ai/rnn/images/vid-fast.gif">
<meta property='og:description' content="This article explains how recurrent neural networks (RNN's) work without using the neural network metaphor. It uses a visually-focused data-transformation perspective to show how RNNs encode variable-length input vectors as fixed-length embeddings."/>
<meta property='og:url' content="http://explained.ai/rnn/index.html"/>

<!-- Facebook meta -->
<meta property="og:type" content="article" />

<!-- Twitter meta -->
<meta name="twitter:title" content="Explaining RNNs without neural networks">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@the_antlr_guy">
<meta name="twitter:creator" content="@the_antlr_guy">
<meta name="twitter:description" content="This article explains how recurrent neural networks (RNN's) work without using the neural network metaphor. It uses a visually-focused data-transformation perspective to show how RNNs encode variable-length input vectors as fixed-length embeddings.">
<meta name="twitter:image" content="http://explained.ai/rnn/images/vid-fast.gif">
<!-- END META -->
</head>
<body>
<div class="watermark">
<i><a href='http://explained.ai/rnn/index.html'>Main article</a><br>Brought to you by <a href='http://explained.ai'>explained.ai</a></i><br>
</div>

<h1>1 An RNN built with matrices and trained with SGD</h1>

<p></p>

<p><a href="https://www.linkedin.com/in/terence-parr/">Terence Parr</a></p>

<p style="font-size: 80%; line-height:1.1;">(Terence is a tech lead at Google and ex-Professor of computer/data science in <a href="https://www.usfca.edu/arts-sciences/graduate-programs/data-science">University of San Francisco's MS in Data Science program</a>. You might know Terence as the creator of the ANTLR parser generator.)</p>



<div id="toc">
<p class="toc_title">Contents</p>
<ul>
	<li><a href="#sec:1.1">The goal: meaningful vectors representing words</a>
	<ul>
	</ul>
	</li>
	<li><a href="#sec:1.2">Encoding words as integers</a>
	<ul>
	</ul>
	</li>
	<li><a href="#sec:1.3">Aggregating character vectors to encode words</a>
	<ul>
	</ul>
	</li>
	<li><a href="#sec:1.4">Encoding and aggregating character vectors through matrix transforms</a>
	<ul>
	</ul>
	</li>
	<li><a href="#sec:1.5">Learning the RNN matrices by training a classifier</a>
	<ul>
	</ul>
	</li>

</ul>
</div>


<p>Recurrent neural networks (RNNs) transform variable-length feature vectors into fixed-length vectors, which is useful because most machine learning models require fixed-sized feature vectors (e.g., the pixels of a 28x28 image or the 10 features of a house for sale). RNNs are, therefore, convenient data preprocessors or model front ends. The key characteristic of the fixed-length vectors generated by RNNs is that they encode information about feature order, which is critical in naturally language processing (NLP) and time series models: &ldquo;man bites dog&rdquo; is very different than &ldquo;dog bites man.&rdquo;  Furthermore, the vectors are learned as part of a regression or classification problem so the vectors are meaningful within the context of that model and training set.  </p>

<p>The section lays the groundwork for building and training RNNs using nothing but matrices and vectors, without the metaphor of neural networks. Rather than derive and compute our own partial derivatives, we'll use PyTorch's automatic differentiation to update model parameters during training.</p>



<h2 id="sec:1.1">1.1 The goal: meaningful vectors representing words</h2>


<p>To understand what's going on inside an RNN, let's re-invent the order-sensitive encodings generated by an RNN using a simple classification problem.  Imagine that we have three words for <i>cat</i> in three different languages. Given a word, we'd like to classify it as English, French, or German:</p>

<p> </p>
<center>
<table style="">
<thead>
<tr>
<th valign=top align="center">Word to word</th><th valign=top align="center">Label-encoded targets</th><th valign=top align="center">Label-encoded features</th>
</tr>
</thead>
<tbody>
<tr>
<td valign=top align="center">

<center>
<center>
<a href="images/word-to-word.svg">
<img src="images/word-to-word.svg" width="100%" url="images/word-to-word.svg">
</a>
</center>

</center>

</td><td valign=top align="center">

<center>
<center>
<a href="images/word-to-num.svg">
<img src="images/word-to-num.svg" width="58%" url="images/word-to-num.svg">
</a>
</center>

</center>

</td><td valign=top align="center">

<center>
<center>
<a href="images/num-to-num.svg">
<img src="images/num-to-num.svg" width="35%" url="images/num-to-num.svg">
</a>
</center>

</center>

</td>
</tr>
</tbody>
</table>
</center>
<p>In order to numericalize data before training a model, we can encode the targets as classes 0, 1, and 2, which works great. Unfortunately, we can't convert the <i>cat</i> words to unique integers because we get nonsense like <img style="vertical-align: -0.5pt;" src="images/eqn-9FCB7D57C355EDBE95FD16562CD8D944-depth000.20.svg">, as shown on the right above.</p>

<p>	 Instead of a single number per word, we need to come up with a vector of numbers to represent each word.  We don't need to know what the vector elements are <i>per se</i>, just that they somehow meaningfully represent a word in some high-dimensional space:</p>

<p><center>
<a href="images/vector-to-num.svg">
<img src="images/vector-to-num.svg" width="20%" url="images/vector-to-num.svg">
</a>
</center>
</p>

<p>As an analogy, consider the common tactic of breaking apart a single date feature (often represented as the number of seconds since 1970) into a vector of its constituent components like hour, minute, day, month, year. </p>

<p>Once we have these meaningful feature vectors (for <i>cat</i>, <i>chat</i>, and <i>katze</i>), we can use them to train a random forest or any other classifier.  So this article is all about how we find suitable vectors. To do that, let's baby step through some possible approaches to arrive at the RNN solution.</p>



<h2 id="sec:1.2">1.2 Encoding words as integers</h2>


<p>The first thing we have to do is split apart the words into a sequence of characters and encode those characters using a character vocabulary.  Computing the vocabulary is straightforward. Any unique integers  will work for the vocabulary, but the implementation is simpler if we use consecutive integers starting from zero:</p>

<p> </p>
<center>
<table style="">
<thead>
<tr>
<th valign=top width="50%" align="center">Vocabulary symbols</th><th valign=top align="center">Words encoded as vectors</th>
</tr>
</thead>
<tbody>
<tr>
<td valign=top align="center">

<center>
<center>
<a href="images/vocab.svg">
<img src="images/vocab.svg" width="40%" url="images/vocab.svg">
</a>
</center>

</center>



<div class="codeblk">vocab = {c:i for i,c in enumerate("acehktz")}</div>

</td><td valign=top align="center">

<center>
<center>
<a href="images/char-encode.svg">
<img src="images/char-encode.svg" width="53%" url="images/char-encode.svg">
</a>
</center>

</center>

</td>
</tr>
</tbody>
</table>
</center>
<p>Unfortunately, the feature vectors for different <i>cat</i> words have different lengths (three, four, and five). A simple way to convert these variable length character sequences into a single feature is to add up the vocabulary character encodings, which we can do with a trivial loop: </p>
<center>
<table style="">
<thead>
<tr>
<th valign=top width="45%" align="center">Character sum encoding loop</th><th valign=top align="center">Words encoded as integers (sum of chars)</th>
</tr>
</thead>
<tbody>
<tr>
<td valign=top align="left">

<div class="codeblk">for x in ["cat,"chat","katze"]:
    x = [vocab[c] for c in x]
    h = 0
    for t in range(len(x)):
        h = h + x[t]</div>

</td><td valign=top align="center">

<center>
<center>
<a href="images/char-sum-encode.png">
<img src="images/char-sum-encode.png" width="41%" url="images/char-sum-encode.png">
</a>
</center>

</center>

</td>
</tr>
</tbody>
</table>
</center>
<p>This is not a great solution: First, because it's unclear that 6, 9, and 14 meaningfully distinguish between the three <i>cat</i> words. More importantly, though, this encoding is order independent. For example, a-c-t has the same encoding as c-a-t:</p>

<p><center>
<a href="images/cat-act.svg">
<img src="images/cat-act.svg" width="13%" url="images/cat-act.svg">
</a>
</center>
</p>

<p>    A simple way to make the encoding order dependent is to multiply previous <span class=inlinecode>h</span> values by, say, 2. This performs a scale and add operation very much like what we'd find in a hash function. Scalar 2 is not some magic number&mdash;I just chose it randomly, but it's a value we could learn if we created and optimized a loss function.</p>
<center>
<table style="">
<thead>
<tr>
<th valign=top width="45%" align="center">Weighted encoding loop</th><th valign=top align="center">Order-dependent word integer encoding</th>
</tr>
</thead>
<tbody>
<tr>
<td valign=top align="left">

<div class="codeblk">for x in ["cat,"chat","katze"]:
    x = [vocab[c] for c in x]
    h = 0
    for t in range(len(x)):
        h = 2 * h + x[t]</div>

</td><td valign=top align="center">

<center>
<center>
<a href="images/cat-act-ordered.png">
<img src="images/cat-act-ordered.png" width="39%" url="images/cat-act-ordered.png">
</a>
</center>

</center>

</td>
</tr>
</tbody>
</table>
</center>
<p>That inner loop is equivalent to the following recurrence relation:</p>

<div><img class="blkeqn" src="images/blkeqn-3048294F838BEC17B89CE0457E8359E8.svg" alt="" url="images/blkeqn-3048294F838BEC17B89CE0457E8359E8.svg"></div>

<p>The recurrence just says that the value of <span class=eqn>h</span> at iteration <span class=eqn>t</span> is twice its previous value plus the encoding of the <img style="vertical-align: -0.5pt;" src="images/eqn-BF4262990CD6522944323C29B8D2353D-depth000.14.svg"> character; <span class=eqn>t</span> moves from 1 to <span class=eqn>m</span> for <span class=eqn>m</span> characters in the word. For <span class=eqn>x</span> = <span class=inlinecode>cat</span> we get three values of <span class=eqn>h</span> beyond our initial value:</p>

<div><img class="blkeqn" src="images/blkeqn-33E117DC61B77D31604D90FAB16799E3.svg" alt="" url="images/blkeqn-33E117DC61B77D31604D90FAB16799E3.svg"></div>

<p>With a little bit of replacement to remove <span class=eqn>h<sub>t</sub></span> values on the right-hand side, we find that the final <span class=eqn>h</span> value is:</p>

<div><img class="blkeqn" src="images/blkeqn-80D2C08B15DB3B525ECC505B91CFB6CC.svg" alt="" url="images/blkeqn-80D2C08B15DB3B525ECC505B91CFB6CC.svg"></div>

<p>The <b>key take away</b> here is that, despite having a constant multiplier of 2, each character encoding is multiplied by a different number, depending on its position in the sequence: <span class=eqn>x<sub>t</sub></span> is multiplied by <img style="vertical-align: -0.5pt;" src="images/eqn-F266CE269952ABC7C03719D70C88E039-depth000.00.svg">.  This is why <i>cat</i> and <i>act</i> get different encodings.  We've solved the order dependency issue, but it's unlikely that a single integer will ever contain enough information to meaningfully represent a natural language word.</p>



<h2 id="sec:1.3">1.3 Aggregating character vectors to encode words</h2>


<p>As we discussed at the beginning, our goal is really to represent entire words by vectors in some higher dimensional space, not integers, which are 1D vectors.  We don't have a proper encoding vector for a word, but we can easily get the one-hot vectors for individual characters.  So, we just have to find a way to convert these character one-hot vectors to a suitable encoding for an entire word. The simplest way to combine the character vectors is to merge them or, equivalently, add them together into a <i>bag of words</i> (BOW), or bag of characters in this case:</p>
<center>
<table style="">
<thead>
<tr>
<th valign=top width="45%" align="center">BOW encoding loop</th><th valign=top align="center">Order-independent word vector encoding</th>
</tr>
</thead>
<tbody>
<tr>
<td valign=top align="left">

<div class="codeblk">for x in ["cat,"chat","katze"]:
    h = torch.zeros(len(vocab), 1)
    for t in range(len(x)):
        h = h + onehot(x[t])</div>

</td><td valign=top align="center">

<center>
<center>
<a href="images/cat-onehot.svg">
<img src="images/cat-onehot.svg" width="70%" url="images/cat-onehot.svg">
</a>
</center>

</center>

</td>
</tr>
</tbody>
</table>
</center>
<p>But, BOW vectors do not encode information about the order of the characters and the summation of a bunch of one-hot vectors isn't much more meaningful than the integer representation.</p>

<p>To combine the character vectors in an order-dependent way, we can simply apply the same trick we did before to scale the previous value of <span class=inlinecode>h</span>. The only difference between this and the previous version is that we use character vectors not character values and we initialize <span class=inlinecode>h</span> to the zero vector rather than 0:</p>
<center>
<table style="">
<thead>
<tr>
<th valign=top width="40%" align="center">Weighted BOW encoding loop</th><th valign=top align="center">Order-dependent word vector encoding</th>
</tr>
</thead>
<tbody>
<tr>
<td valign=top align="left">

<div class="codeblk">for x in ["cat,"chat","katze"]:
    h = torch.zeros(len(vocab), 1)
    for t in range(len(x)):
        h = 2 * h + onehot(x[t])</div>

</td><td valign=top align="center">

<center>
<center>
<a href="images/cat-onehot-ordered.svg">
<img src="images/cat-onehot-ordered.svg" width="75%" url="images/cat-onehot-ordered.svg">
</a>
</center>

</center>

</td>
</tr>
</tbody>
</table>
</center>
<p>Turning the crank three times yields <img style="vertical-align: -2.1954868pt;" src="images/eqn-9B3C2A82E196EF44D1ADF9E2D25F59B4-depth002.09.svg"> as our encoding for <i>cat</i>. Again, multiplying by 2 is nonsense, but the bigger issue is that multiplying by any single scalar is unlikely create a word encoding  meaningful enough to distinguish these words properly.</p>

<p>To make a more sophisticated model, we need to multiply the <span class=inlinecode>h</span> vector by a matrix <span class=eqn>W</span> (<span class=inlinecode>@</span> is the matrix multiply operator), which represents many more model parameters than a lonely scalar value.  There are multiple useful interpretations of matrix-vector multiplication, but it makes the most sense in this case to think of matrix-vector multiplication as transforming a vector into a new space (possibly with different dimensionality). Let's start with the identity matrix as the transformation matrix to see what it looks like:</p>
<center>
<table style="">
<thead>
<tr>
<th valign=top width="50%" align="center">Matrix transform BOW encoding loop</th><th valign=top align="center">Order-dependent word vector encoding</th>
</tr>
</thead>
<tbody>
<tr>
<td valign=top align="left">

<div class="codeblk">nfeatures = len(vocab)
W = torch.eye(nfeatures,   nfeatures)
h = torch.zeros(nfeatures, 1)
for x in ["cat,"chat","katze"]:
    h = torch.zeros(len(vocab), 1)
    for t in range(len(x)):
        h = W @ h + onehot(x[t])</div>

</td><td valign=top align="center">

<center>
<center>
<a href="images/equation-identity.svg">
<img src="images/equation-identity.svg" width="70%" url="images/equation-identity.svg">
</a>
</center>

</center>

</td>
</tr>
</tbody>
</table>
</center>
<p>(Section <a href="https://www.amazon.com/Grokking-Deep-Learning-Andrew-Trask/dp/1617293709">Using identity vectors to sum word embeddings</a> in Trask Chapter 12 is a nice section on this topic.)</p>

<p>Because  multiplying a vector by the identity matrix leaves the vector as-is, the update equation reduces to <span class=inlinecode>h = h + onehot(c)</span>, which leads to a BOW word encoding because it just sums up the character one-hot vectors. </p>



<h2 id="sec:1.4">1.4 Encoding and aggregating character vectors through matrix transforms</h2>


<p>So <span class=eqn>W</span> can start as the identity or random values, but we need to learn appropriate values for the elements of that matrix. In order to do that, the <span class=inlinecode>h</span> update equation applying <span class=eqn>W</span> must be part of a classification (or regression) problem. The <span class=eqn>W</span> elements we learn will be appropriate  for a specific classification problem, although often <span class=inlinecode>h</span> vectors are useful for transfer learning purposes. For example, I often use the <a href="https://nlp.stanford.edu/projects/glove">GloVe word vectors</a> from Stanford as a starting point to encode words as vectors. </p>

<p>There's one more issue to deal with that makes our update equation a bit more complicated:  As it is now, the dimensions of <span class=eqn>W</span> must be the same size as the vocabulary, which could be in the hundreds of thousands, if we were dealing with words not characters.  If the  vocabulary were 100,000, <span class=eqn>W</span> would have 10 billion elements, which  would require 40GB on a CPU or GPU using 32-bit floating-point numbers.  We'd prefer a much smaller <span class=eqn>W</span>, on the order of 100 x 100, depending on the problem. </p>

<p>To decouple the size of <span class=eqn>W</span> from the vocabulary size, let's introduce another matrix, <span class=eqn>U</span>, that transforms the <span class=eqn>x<sub>t</sub></span>'s sparse one-hot vector from the size of the vocabulary to a dense vector of length <span class=inlinecode>nhidden</span>. I've chosen <span class=inlinecode>nhidden</span> to be four here to keep the diagrams small and because we have such a small training set to learn:</p>
<center>
<table style="">
<thead>
<tr>
<th valign=top width="50%" align="center">Matrix transform BOW encoding loop</th><th valign=top align="center">Order-dependent word vector encoding</th>
</tr>
</thead>
<tbody>
<tr>
<td valign=top align="left">

<div class="codeblk">nhidden=4; nfeatures=len(vocab)
W = torch.eye(nhidden,   nhidden)
U = torch.randn(nhidden, nfeatures)
for x in ["cat,"chat","katze"]:
    h = torch.zeros(nhidden, 1)
    for t in range(len(x)):
	    h = W @ h + U @ onehot(x[t])</div>

</td><td valign=top align="center">

<center>
<center>
<a href="images/equation-W-U.svg">
<img src="images/equation-W-U.svg" width="90%" url="images/equation-W-U.svg">
</a>
</center>

</center>

</td>
</tr>
</tbody>
</table>
</center>
<p>(Matrices <span class=eqn>W</span>, <span class=eqn>U</span> are called <span class=eqn>W<sub>hh</sub></span>, <span class=eqn>W<sub>xh</sub></span> in Karpathy's article and <span class=inlinecode>h_h</span>, <span class=inlinecode>i_h</span> in Section &ldquo;Our Language Model in PyTorch&rdquo; of <a href="https://github.com/fastai/fastbook/blob/master/12_nlp_dive.ipynb">Chapter 12 in the fastai book</a>.)</p>

<p>By transforming <span class=eqn>x<sub>t</sub></span>'s one-hot vector into a smaller dense vector using <span class=eqn>U</span>, the <span class=eqn>W</span> matrix can be as small or big as we want. The cool thing is that the <span class=eqn>U</span> matrix acts like an embedding layer for a neural network. Multiplying <span class=eqn>U</span> times the <span class=eqn>x<sub>t</sub></span>'s one-hot vector effectively selects a column from <span class=eqn>U</span>. So, the columns of <span class=eqn>U</span> are the character vector embeddings after we train a model containing this <span class=inlinecode>h</span> update equation.</p>

<p>As before with the scalar BOW encoding loop, if <span class=eqn>W</span> is the identity, then we are just adding up vector representations of the input characters. In this case, however, we would not be summing up the one-hot representations. Instead, we'd be summing up the embeddings for <span class=eqn>x<sub>t</sub></span>, which gives us a <i>continuous bag of words</i> (CBOW), not a BOW. CBOW simply means an aggregate of multiple symbol vectors that does not encode symbol order information. </p>
<div class=aside><b>CBOW term confusion</b><br>
There is a bit of confusion surrounding the term CBOW from what I can tell reading articles on the net. People often use plain CBOW to refer to the model from the <a href="https://arxiv.org/pdf/1301.3781.pdf">original word2vec paper</a> that predicted the middle focus word from surrounding context words: &ldquo;<i>We denote this model further as CBOW, as unlike standard bag-of-words model, it uses continuous distributed representation of the context.</i>&rdquo;  My interpretation is that CBOW refers to aggregate embeddings but &ldquo;CBOW model&rdquo; refers to the model in the word2vec paper that used focus words and context.
</div>
<p>It's easier to internalize this loop if we unroll it and watch it in action to see how the <span class=inlinecode>h</span> partial results vectors get computed:</p>

<p><center>
<a href="images/unrolled.gif">
<img src="images/unrolled.gif" width="35%" url="images/unrolled.gif">
</a>
</center>
</p>

<p>Notice that <span class=eqn>W</span> and <span class=eqn>U</span> do not change as we compute the <span class=inlinecode>h</span> partial results vector for a word. That's an important point, and we'll discuss how and when those matrices change below.  It's analogous to when we multiplied vectors by magic constant 2 above. Despite multiplying <span class=inlinecode>h</span> by a constant matrix, each one-hot input vector is multiplied by a different matrix.</p>

<p>There's one more detail we need in the transformation loop. We can't just sum up the <span class=inlinecode>h</span> partial results. Each <span class=inlinecode>h</span> has to pass through a nonlinearity first, which will ultimately increase the predictive strength of the model but also ensures <span class=inlinecode>h</span> stays within -1..1:</p>


<div class="codeblk">for x in ["cat,"chat","katze"]:
    h = torch.zeros(nhidden, 1)
    for t in range(len(x)):
        h = W @ h + U @ onehot(x[t])
        h = torch.tanh(h) # NONLINEARITY, contrains h values to -1..1</div>


<p>(The <a href="https://github.com/fastai/fastbook/blob/master/12_nlp_dive.ipynb">fastai book</a> uses <span class=inlinecode>relu</span> not <span class=inlinecode>tanh</span>, such as in <span class=inlinecode>LMModel1</span> of Section &ldquo;Our Language Model in PyTorch&rdquo;, but that caused my <span class=inlinecode>h</span> vectors to explode in magnitude. I don't think it has anything to do with exploding gradients because I'm under the impression that <span class=inlinecode>tanh</span> suffers from vanishing gradients not exploding. I guess this due to a lack of L2 weight decay regularization in my training loop. Karpathy's snippet also uses <span class=inlinecode>tanh</span>. Update: I figured it out. softmax followed by cross entropy has poor numerical characteristics. When I use pytorch's <span class=inlinecode>cross_entropy()</span> that combines them, <span class=inlinecode>relu</span> seems to work very well. I also added a zero-initialized bias term in the <span class=inlinecode>h</span> update equation! Clipping the gradient values to 1.0 or similar also helps a lot.)</p>



<h2 id="sec:1.5">1.5 Learning the RNN matrices by training a classifier</h2>


<p>The <span class=inlinecode>h</span> computed by the inner loop yields an embedding vector for each word, computed from the sequence of character one-hot vectors for that word. That is just a representation of a word, which we could use in a random forest or any other classifier to make predictions (given suitable <span class=eqn>W</span> and <span class=eqn>U</span>).  To learn the values in <span class=eqn>W</span> and <span class=eqn>U</span>, though, we have to make the matrices part of a classifier model. Given a suitable loss function, we can tweak the values of <span class=eqn>W</span> and <span class=eqn>U</span> in the direction of lower loss, yielding more and more accurate predictions.</p>

<p>To make a prediction from word vector <span class=inlinecode>h</span>, we again use a matrix transformation that represents either a final linear layer of a neural network or a multi-class logistic regression model, depending on your preference. We transform <span class=inlinecode>h</span> to a vector of length 3 because there are 3 target languages: English, French, and German. We'll use a new matrix, <span class=eqn>V</span>, to transform the final <span class=inlinecode>h</span> to the output of our classifier model.  The last two lines of this code snippet represent the classifier:</p>
<center>
<table style="">
<thead>
<tr>
<th valign=top width="50%" align="center">Matrix transform BOW encoding loop</th><th valign=top align="center">Order-dependent word vector encoding</th>
</tr>
</thead>
<tbody>
<tr>
<td valign=top align="left">

<div class="codeblk">nhidden=4; nfeatures=len(vocab); nclasses=3
W = torch.eye(nhidden,    nhidden)
U = torch.randn(nhidden,  nfeatures)
V = torch.randn(nclasses, nhidden)
for x in ["cat,"chat","katze"]:
   h = torch.zeros(nhidden, 1)
   for t in range(len(x)):
       h = W @ h + U @ onehot(x[t])
       h = torch.tanh(h)
	   
    o = V @ h      # Get non-normalized scores
    o = softmax(o) # Normalize to sum to 1.0</div>

</td><td valign=top align="center">

<center>
<center>
<a href="images/equation-W-U-V.svg">
<img src="images/equation-W-U-V.svg" width="95%" url="images/equation-W-U-V.svg">
</a>
</center>

</center>

</td>
</tr>
</tbody>
</table>
</center>
<p>(This code corresponds to models <span class=inlinecode>RNN</span> in Karpathy and <span class=inlinecode>LMModel2</span> in Section &ldquo;Our First Recurrent Neural Network&rdquo; of <a href="https://github.com/fastai/fastbook/blob/master/12_nlp_dive.ipynb">Chapter 12 in the fastai book</a>. The fastai book uses matrix <span class=inlinecode>h_o</span> instead of <span class=eqn>V</span>, and Karpathy uses matrix variable <span class=inlinecode>Why</span>.)</p>
<div class=aside><b>Simplifications made in this code</b><br>
Some readers will notice that I'm not using any bias vectors in this code, which are typically included in neural network layers and logistic regression models. It turns out to work pretty well without them for this application and simplifies the code a lot.
<p>	We also should shuffle the <span class=inlinecode>X</span> training data rather than go through it in the same order each epoch.</p>

</div>
<p>The loop that we have so far can make predictions for every record in our training set,  but it does not learn. It just keeps repeating the same predictions over and over because <span class=eqn>W</span>, <span class=eqn>U</span>, and <span class=eqn>V</span> do not change. In order to improve predictions and get better word vectors, our training loop has to update <span class=eqn>W</span>, <span class=eqn>U</span>, and <span class=eqn>V</span>, which we can do with stochastic gradient descent (SGD). SGD has lots of variations, but the stochastic part comes from the fact that we are updating the model parameters after making a prediction from one (or just a few) records. Let's add the computation of our loss function and pseudocode to update our matrices:</p>
<center>
<table style="">
<thead>
<tr>
<th valign=top width="50%" align="center">RNN training loop</th><th valign=top align="center">Animation</th>
</tr>
</thead>
<tbody>
<tr>
<td valign=top align="left">

<div class="codeblk">init W, U, V
for i in range(0, len(X)):
    x = X[i]

    h = torch.zeros(nhidden, 1)
    for t in range(len(x)):
       h = W@h + U@onehot(x[t])
       h = torch.relu(h)
    o = V@h
    o = softmax(o)

    loss = cross_entropy(o, y[i])
    update W,U,V towards lower loss</div>

</td><td valign=top align="center"><iframe src="https://www.youtube.com/embed/ppz0XdEcGF4?autoplay=0&rel=0" width="380"  frameborder="0" allow="accelerometer; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></td>
</tr>
</tbody>
</table>
</center>
<p>In PyTorch, the update pseudocode line would look something like:</p>


<div class="codeblk">loss = cross_entropy(o, y_train[i])
optimizer.zero_grad()
loss.backward() # autograd computes W.grad, U.grad, ...
optimizer.step()</div>


<p>where <span class=inlinecode>optimizer</span> is an <span class=inlinecode>Adam</span> or <span class=inlinecode>RMSProp</span> optimizer,  for example.</p>

<p>At this point, we've created an RNN that uses SGD to update <span class=eqn>W</span>, <span class=eqn>U</span>, <span class=eqn>V</span> after each input record.  You can check out the full implementation in the <a href="https://colab.research.google.com/github/parrt/ml-articles/blob/master/rnn/notebooks/SGD.ipynb">SGD notebook</a>.   This training procedure works well but is very inefficient because computing the partial derivatives during back propagation is very expensive, and we do it after each record. To improve performance without sacrificing much in training accuracy, we can train a small batch of records before updating the matrices. That's called <i>minibatch</i> SGD, and is the subject of the <a href="minibatch.html">next section</a>.</p>



</body>
</html>
